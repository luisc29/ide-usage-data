\chapter{Related work}

\section{Studies on programmers' activities}

\section{Work fragmentation}


\section{Mining Software Repositories}
The core of this work is within the Mining Software Repositories research area, which can be defined as the extraction of information from different artifacts (e.g. source code, control version logs, documentation and bug reports) that are produced throughout the software development cycle \cite{e}. This term is not alien to data mining, so the intentions are extracting knowledge and discovering patterns in the data using a set of techniques and algorithms for this goal \cite{e}.

The software repositories can be found in different formats:
\begin{itemize}
	\item \textbf{Historical repositories}, with information obtained throughout the evolution of a project like bug reports, emails and control version logs.
	\item \textbf{Runtime repositories}, like deployment information, performance reports and application usage data.
	\item \textbf{Code repositories}, which is access to the source code from a control version tool like SourceForge or Github.
\end{itemize}

Although some of this repositories are used to keep control of changes and procedures, they are rarely used to make decisions. One of the goals of the Mining Software Repositories field is allowing static repositories to be a guide for decision making, for it is possible to discover important information and patterns that can help predict the future performance of the team and the quality of the product in development \cite{e}. Each stage of the development of software releases data that can be exploited to make timely decisions. Recently, it was appointed the name of \emph{Software Intelligence} for all the practices involving Mining Software Repositories within companies \cite{e}.

\section{Predicting errors}
There are tasks that have a greater impact with these practices. For example, the project administration can make decisions based on facts and tendencies reflected in the data that can be visualized with metrics, or even predict future events like modules prone to errors based on recent activity \cite{5}

Related to the latter, there is a great effort in using this data for quality assurance. Khomf et al. (2012) analyzed product release data of the Firefox navigator while the developers were migrating from a traditional release scheme to an agile methodology and found that, though there are the same amount of errors, the users tend to identify them early and the failures are fixed quicker.

Error prediction is another example of the interest in quality. The work made by Nagappan et al. (2006) and Finaly (2014) are similar approaches of error prediction using complexity metrics, which are measures to the source code like the number of lines, functions, classes and more. Both works agree about the difficulty of predicting errors using solely this metrics and in the low precision of the proposed models. Zimmerman and Nagappan (2008) obtained better results considering the structure of the system and the relation between classes, creating a network and obtaining metrics from the related graph. The graph was built identifying the code dependencies of the program, where the nodes are classes and the edges the dependencies between them. From the graph they obtained metrics like the number of nodes, the amount of dependencies in relation to a node and the distance to a node. They found that, although the complexity metrics have better correlation with the amount of errors, the graph metrics can predict well with regression models.

Meneely et al. (2008) also used networks to predict errors in modules based on product release data, but instead of representing classes as nodes, they represent the programmers that worked on the project and the edges are created when two of them work in the same piece of code. The authors also used graph metrics to train a predictive model, obtaining a precision of 81\%.

\section{Interaction data programmer-IDE}
Another example of data that can be captured is the interaction data (also called usage data) between the programmer and the development tools \cite{SnipesETALASD}, which are basically a log of the execution of events within the IDE (Integrated Development Environment). The IDE provide tools to execute tasks effectively, like navigation between classes and methods, continuous compilation, refactoring, automated testing and debugging, all designed to assist the programmers' productivity.

The usage data is composed of the interactions between the programmer and the IDE, and include the description of executed commands (e.g. copy, paste, delete and save); the opening or closing of files; clicks, change of line and navigation around text; usage of tools, and more. Generally, everything is stored in a log with date, time and an identifier for the user. As a complement it can contain the name of the class and/or the function where it was executed and the type of the event.

\section{Capturing usage data}
It is possible to command the IDE to capture usage data to have better understanding, to a low level, of the activities of the programmer \cite{34}. Eclipse and Visual Studio are examples of IDEs that provide an API (Application Programming Interface) that allows to register all the commands and actions that are begin executed. It is not necessary to build an application to capture the data, for there are several options to collect it like Eclipse Usage Data Collector, Mylyn Monitor and Codealike. It is possible to start a study with existing data from previous projects. An example is the available information in the Eclipse archives about the Usage Data Collector, a dataset of more than 1 million users and 2,000 million registered events.

\section{Productivity metrics}
Given that the available information about usage data can tell the history of activity of the programmer, one of the most used metrics is the total active time, without the time consumed in interruptions, like the study by Sanchez et al. (2015). The active time is also considered by the focus function of Codealike, which increases when more activity is registered. The focus level is a metric used by this tool to measure the productivity and tries to model the concentration of the user. Prolonged time without registered activity o time outside the IDE are identified as interruptions, being the number of interruptions another metric.
When it is possible to classify the events by its type it can be used metrics like the editions per minute or selections per minute. It is also common to use debugging per minute as metric, like the research by Carter and Dewan (2010). Moreover, Sanchez et al. (2015) identified that during sessions without interruptions the proportion of edition events is superior than navigation events, so one of their conclusions is that it is a good indicator of productivity. This tell us that a productive programmer spends more time coding and less time exploring the code.
Codealike also uses a classification of events to quantify the technical debt when a class or function has more navigation events and debugging than edition events, which indicates that the element can be a bottle neck for the project. 

\section{Research with usage data}
Kersten and Murphy (2006) propose a tool that keep the context of the task being performed by the programmer and make it visible to help in the navigation around elements. The context is a graph of classes, modules and functions that are relevant for the current task and that are needed to complete it. Each element related to the task has a weighted value that is created with a model of the degree of interest on that element according to the task, and it is shown to the programmer a list of elements ordered by the value. In a field study with programmers, the authors obtained qualitative and quantitative results that indicate an increase of productivity when having context of the task in progress, specially in big systems with many programmers collaborating.

Fritz et al. (2007) did a research about the possibility of inferring weather the programmer has knowledge of the code or not by quantifying the degree of interest, which is the amount of recent activity that the programmer made on an element, similar to the work done by Kersten and Murphy (2006). The degree of interest increases with the frequency of interactions on an element and decreases when the interactions stop. In a field study, they monitored the activity of 19 programmers and applied a weekly survey for three weeks. They concluded that when the programmer has knowledge of code (according to the surveys) there is a high degree of interest, concluding that the usage data can be used to identify this phenomenon.

Carter and Dewan (2010) inquire about the possibility of identifying a programmer stuck or having problems, which is when there is no progress despite of the effort, under the hypothesis that the activity of the programmer will give out evidence of this state. During an experiment with students in a programming course, the participants were asked to indicate when they were facing problems during programming tasks and while usage data was being captured. From the data they obtained metrics of the number of edition, navigation and debugging events per minute, and with the data labeled where the programmer was stuck, they trained several machine learning algorithms. The best result was obtained with a decision tree, which correctly predicted the "in problems" state the 92\% of the times, concluding that it is possible to identify this state with usage data.

Minelli et al. (2004) performed an analysis about the process of program comprehension and compared the results with the literature. Based on interaction data, they labeled the segments of the working sessions of programmers according to the activity, specifically in sectors where the programmer was navigating around classes, editing code, inspecting objects and reading the code. Then, they split the sessions in segments and quantified them according to the activity performed and concluded that the program comprehension activity was underestimated by previous research, for the results indicated that this activity covers (in average) 65\% to 90\% of the working sessions, against a previous estimation of 35\%.

Sanchez et al. (2015) used interaction data to perform an empirical analysis of the impact of work fragmentation on productivity, identifying lapses of time without recorded activity as interruptions and quantifying the productivity according to the number of edition and selection events, and the proportion of edition events against selections. They found that productivity decreases as the number of interruptions increases in a working session, and that in sessions with prolonged interruptions the productivity tends to be lesser in comparison with sessions with only short or none interruption. In session without interruptions the productivity is triplicated, but they are only the 2\% of all the sessions of a programmer.
